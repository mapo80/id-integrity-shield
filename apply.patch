diff --git a/idtamper/profiles.py b/idtamper/profiles.py
index 1111111..2222222 100644
--- a/idtamper/profiles.py
+++ b/idtamper/profiles.py
@@ -1,11 +1,68 @@
-import json
-from pathlib import Path
+import os, json
+from pathlib import Path
+from typing import Any, Dict
 
-def load_profile(name_or_path: str):
-    here = Path(__file__).parent.parent / "profiles"
-    p = Path(name_or_path)
-    if p.suffix == ".json" and p.exists():
-        return json.loads(p.read_text())
-    return json.loads((here/f"{name_or_path}.json").read_text())
+PROFILES_DIR = Path(os.getenv("IDS_PROFILES_DIR", "/app/profiles"))
+
+def _read_json(p: Path) -> Dict[str, Any]:
+    return json.loads(p.read_text(encoding="utf-8"))
+
+def load_profile(name_or_path: str) -> Dict[str, Any]:
+    """
+    Caricamento profili robusto:
+      - Accetta path assoluto/relativo o solo nome senza .json
+      - Supporta alias con suffisso '@N' → fallback al core (es. recapture-id@2 → recapture-id) se il file non esiste
+      - Directory configurabile via IDS_PROFILES_DIR (default: /app/profiles)
+    """
+    p = Path(name_or_path)
+    # 1) path esplicito
+    if p.suffix == ".json" and p.exists():
+        return _read_json(p)
+    if p.is_absolute() and p.exists():
+        return _read_json(p)
+
+    # 2) prova nome esatto nella dir profili
+    stem = p.name[:-5] if p.name.endswith(".json") else p.name
+    cand = PROFILES_DIR / f"{stem}.json"
+    if cand.exists():
+        return _read_json(cand)
+
+    # 3) fallback alias: recapture-id@2 -> recapture-id
+    core = stem.split("@", 1)[0]
+    core_cand = PROFILES_DIR / f"{core}.json"
+    if core and core_cand.exists():
+        return _read_json(core_cand)
+
+    # 4) errore con elenco disponibili
+    available = sorted(x.name for x in PROFILES_DIR.glob("*.json"))
+    raise FileNotFoundError(
+        f"Profile '{name_or_path}' not found. Looked in {PROFILES_DIR}. Available: {available}"
+    )
diff --git a/app/main.py b/app/main.py
index 3333333..4444444 100644
--- a/app/main.py
+++ b/app/main.py
@@ -1,17 +1,23 @@
 from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Depends, Request
 from fastapi.responses import JSONResponse, FileResponse
 from fastapi.security.api_key import APIKeyHeader
 from pydantic import BaseModel
-import os, shutil, uuid, json, logging, time
+import os, uuid, json, logging, time
 from pathlib import Path
 from typing import Optional, Dict, Any
 
 from prometheus_fastapi_instrumentator import Instrumentator
 from idtamper.pipeline import analyze_image, AnalyzerConfig
 from idtamper.profiles import load_profile
 
 API_KEY_NAME = "x-api-key"
 api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)
 
+# ----- Path dati e modelli (configurabili via env) -----
+DATA_DIR = Path(os.getenv("DATA_DIR", "/app/data"))
+INCOMING_DIR = DATA_DIR / "incoming"
+RUNS_DIR = DATA_DIR / "runs"
+MODELS_DIR = Path(os.getenv("IDS_MODELS_DIR", "/app/models"))
+for _d in (INCOMING_DIR, RUNS_DIR):
+    _d.mkdir(parents=True, exist_ok=True)
 
 def get_api_key(api_key: str = Depends(api_key_header)):
     expected = os.environ.get("API_KEY")
@@ -84,6 +90,18 @@ class AnalyzeResponse(BaseModel):
     per_check: Dict[str, Any]
     artifacts: Dict[str, str]
 
+# --- registry modelli interno all’app (profili NON devono contenere i path) ---
+# se rinomini i file modello nell’immagine, aggiorna qui (o via env IDS_*_MODEL)
+DEFAULT_MODEL_REGISTRY = {
+    "mantranet":   os.getenv("IDS_MANTRANET_MODEL",   str(MODELS_DIR / "mantranet.onnx")),
+    "noiseprintpp":os.getenv("IDS_NOISEPRINT_MODEL",  str(MODELS_DIR / "noiseprintpp.onnx")),
+}
+def _resolve_model_path(model_path: Optional[str]) -> Optional[str]:
+    if not model_path:
+        return None
+    p = Path(model_path)
+    return str(p if p.is_absolute() else MODELS_DIR / p)
+
 @app.post("/v1/analyze", response_model=AnalyzeResponse)
 async def analyze_endpoint(
     file: UploadFile = File(...),
@@ -94,34 +112,84 @@ async def analyze_endpoint(
     thresholds_json: Optional[str] = Form(None),
     save_artifacts: bool = Form(True),
     _api_key: str = Depends(get_api_key)
 ):
-    tmp_root = Path("/data/incoming"); tmp_root.mkdir(parents=True, exist_ok=True)
-    item_id = str(uuid.uuid4())[:8]
-    img_path = tmp_root / f"{item_id}_{file.filename}"
+    # --- salva upload in path scrivibile ---
+    item_id = str(uuid.uuid4())[:8]
+    img_path = INCOMING_DIR / f"{item_id}_{file.filename}"
     with img_path.open("wb") as f:
         f.write(await file.read())
 
-    prof = load_profile(profile)
-    params = prof["params"]
-    if params_json:
-        user_params = json.loads(params_json)
-        # shallow merge: override specific trees
-        for k,v in user_params.items():
-            params[k] = {**params.get(k, {}), **v} if isinstance(v, dict) else v
-    thresholds = prof["thresholds"]
-    if thresholds_json:
-        thr_user = json.loads(thresholds_json)
-        thresholds.update(thr_user)
-
-    cfg = AnalyzerConfig(weights=prof["weights"], threshold=prof["threshold"], check_params=params, check_thresholds=thresholds)
-    out = Path(out_dir) if out_dir else Path("/data/runs")/item_id
+    prof = load_profile(profile)
+
+    # ----- Params: partiamo da quelli del profilo (seci sono), poi override da form -----
+    params: Dict[str, Any] = dict(prof.get("params", {}))
+    if params_json:
+        user_params = json.loads(params_json)
+        for k, v in user_params.items():
+            params[k] = {**params.get(k, {}), **v} if isinstance(v, dict) else v
+
+    # ----- Inietta i path dei modelli in base ai check abilitati (se mancanti) -----
+    checks = prof.get("checks", {})
+    for check_name, default_path in DEFAULT_MODEL_REGISTRY.items():
+        chk_cfg = checks.get(check_name)
+        if not (isinstance(chk_cfg, dict) and chk_cfg.get("enabled")):
+            continue
+        params.setdefault(check_name, {})
+        params[check_name].setdefault("model_path", default_path)
+        # normalizza vs MODELS_DIR se qualcuno ha passato un relativo
+        params[check_name]["model_path"] = _resolve_model_path(params[check_name]["model_path"])
+        # existence check
+        mp = Path(params[check_name]["model_path"])
+        if not mp.exists():
+            raise HTTPException(
+                status_code=500,
+                detail=f"Model for '{check_name}' not found at {mp}. "
+                       f"Configure env IDS_MODELS_DIR or IDS_*_MODEL, or mount the file."
+            )
+
+    # ----- Thresholds: se non presenti, derivali dai checks -----
+    if "thresholds" in prof and isinstance(prof["thresholds"], dict):
+        thresholds: Dict[str, float] = dict(prof["thresholds"])
+    else:
+        thresholds = {
+            name: (cfg.get("threshold", 0.5) if isinstance(cfg, dict) else 0.5)
+            for name, cfg in checks.items() if isinstance(cfg, dict)
+        }
+    if thresholds_json:
+        thr_user = json.loads(thresholds_json)
+        thresholds.update(thr_user)
+
+    # ----- Threshold globale: supporta prof.decision.threshold o prof.threshold (legacy) -----
+    decision = prof.get("decision") or {}
+    global_threshold = decision.get("threshold", prof.get("threshold", 0.5))
+
+    # ----- Modello principale per AnalyzerConfig (niente più 'weights' nel profilo) -----
+    # Strategia: usa 'mantranet' se abilitato, altrimenti 'noiseprintpp', altrimenti None → errore
+    main_model = None
+    for cand in ("mantranet", "noiseprintpp"):
+        if cand in checks and checks[cand].get("enabled"):
+            main_model = params.get(cand, {}).get("model_path")
+            if main_model:
+                break
+    if not main_model:
+        raise HTTPException(
+            status_code=500,
+            detail="No main ONNX model resolved (expected 'mantranet' or 'noiseprintpp'). "
+                   "Enable one of these checks in the profile."
+        )
+
+    cfg = AnalyzerConfig(
+        weights=main_model,                 # ← rimane il nome del parametro, ma NON viene più dal profilo
+        threshold=global_threshold,
+        check_params=params,
+        check_thresholds=thresholds
+    )
+
+    out = Path(out_dir) if out_dir else RUNS_DIR / item_id
     out.mkdir(parents=True, exist_ok=True)
     rep = analyze_image(str(img_path), str(out), cfg)
 
     if not save_artifacts:
         rep["artifacts"] = {}
     return JSONResponse(rep)
 
diff --git a/tests/test_api_profile_no_weights.py b/tests/test_api_profile_no_weights.py
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/tests/test_api_profile_no_weights.py
@@ -0,0 +1,91 @@
+import os, json
+from pathlib import Path
+from fastapi.testclient import TestClient
+from app.main import app, MODELS_DIR
+
+def test_profile_without_weights_works(tmp_path, monkeypatch):
+    """
+    Verifica che un profilo senza 'weights' e senza 'params.*.model_path'
+    funzioni perché i path modello vengono iniettati dal backend.
+    """
+    client = TestClient(app)
+
+    # Modelli finti
+    models_dir = tmp_path / "models"
+    models_dir.mkdir(parents=True, exist_ok=True)
+    (models_dir / "mantranet.onnx").write_bytes(b"\x00")
+    (models_dir / "noiseprintpp.onnx").write_bytes(b"\x00")
+    monkeypatch.setenv("IDS_MODELS_DIR", str(models_dir))
+
+    # Profilo minimale nel tmp e override IDS_PROFILES_DIR
+    profiles_dir = tmp_path / "profiles"
+    profiles_dir.mkdir(parents=True, exist_ok=True)
+    prof = {
+        "profile_id": "recapture-id@2",
+        "checks": {
+            "mantranet": {"enabled": True, "weight": 0.5, "threshold": 0.55},
+            "noiseprintpp": {"enabled": True, "weight": 0.03, "threshold": 0.65},
+        },
+        "decision": {"metric": "tamper_score", "threshold": 0.62, "policy": "score"}
+    }
+    (profiles_dir / "recapture-id.json").write_text(json.dumps(prof), encoding="utf-8")
+    monkeypatch.setenv("IDS_PROFILES_DIR", str(profiles_dir))
+
+    # immagine finta
+    img = tmp_path / "img.png"
+    img.write_bytes(b"\x89PNG\r\n")
+
+    with img.open("rb") as f:
+        r = client.post(
+            "/v1/analyze",
+            files={"file": ("img.png", f, "image/png")},
+            data={"profile": "recapture-id@2"}  # userà il fallback a recapture-id
+        )
+    # Non verifichiamo il contenuto (dipende dalla pipeline), ma che non esploda per mancanza pesi
+    assert r.status_code in (200, 500)  # 200 se pipeline mock-friendly, 500 se ONNX inferenza fallisce
+    # Importante: l’errore 500 non deve essere per 'weights' mancanti
+    if r.status_code == 500:
+        assert "weights" not in (r.text or "")
diff --git a/README.md b/README.md
index 6666666..7777777 100644
--- a/README.md
+++ b/README.md
@@ -140,6 +140,25 @@ Response: JSON with scores, confidence, per-check details, artifact paths.
 
 * * *
 
+## Model resolution (no `weights` in profiles)
+
+Profiles **must not** embed model paths. The backend injects model locations automatically based on enabled checks:
+
+```python
+DEFAULT_MODEL_REGISTRY = {
+  "mantranet":    "/app/models/mantranet.onnx",
+  "noiseprintpp": "/app/models/noiseprintpp.onnx",
+}
+```
+
+Customize via environment:
+
+```
+IDS_MODELS_DIR=/custom/models
+IDS_MANTRANET_MODEL=/custom/models/my_mantra.onnx
+IDS_NOISEPRINT_MODEL=/custom/models/noisepp.onnx
+```
+
 ## Docker Usage
 
     docker build -t id-integrity-shield:cpu .
